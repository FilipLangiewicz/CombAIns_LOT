{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60b58e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from L_score import L_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1db225dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HackatonDataset(Dataset):\n",
    "    def __init__(self, X, y_df):\n",
    "        \"\"\"\n",
    "        Konstruktor przyjmuje macierz cech (X) oraz DataFrame y_df,\n",
    "        w którym kolumna 'label' to etykieta layoutu (0..9),\n",
    "        a 'clicked' to informacja aposteriori (0 lub 1).\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y_df[\"label\"].values, dtype=torch.long)\n",
    "        self.clicked = torch.tensor(y_df[\"clicked\"].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.clicked[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f36a9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ClickModel, self).__init__()\n",
    "        # Zakładamy, że label traktujemy jako pojedynczą cechę liczbową, więc dodajemy 1 do input_dim\n",
    "        self.fc = nn.Linear(input_dim + 1, 1)\n",
    "\n",
    "    def forward(self, x, layout_idx):\n",
    "        \"\"\"\n",
    "        x: tensor o kształcie [batch_size, input_dim]\n",
    "        layout_idx: tensor [batch_size] (typ long), czyli bezpośrednio numer layoutu\n",
    "        \"\"\"\n",
    "        # Konwertujemy layout na typ float i rozszerzamy wymiar, by miał kształt [batch_size, 1]\n",
    "        layout_feature = layout_idx.unsqueeze(1).float()\n",
    "        # Łączymy wektor cech x z cechą layoutu\n",
    "        combined = torch.cat([x, layout_feature], dim=1)\n",
    "        # Przewidujemy prawdopodobieństwo kliknięcia\n",
    "        p = torch.sigmoid(self.fc(combined))\n",
    "        return p  # zwracamy tensor o kształcie [batch_size, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb72d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_fn( ##### nowa dynamic_lambdas !!!!!\n",
    "    layout_logits,   # [batch_size, 10]\n",
    "    true_layout,     # [batch_size]\n",
    "    clicked,         # [batch_size], 0/1\n",
    "    X_batch,         # [batch_size, input_dim]\n",
    "    click_model,\n",
    "    lambda_val# model przewidujący p=clicked\n",
    "):\n",
    "    \"\"\"\n",
    "    Zwraca średni loss:\n",
    "      - cross entropy dla layoutu\n",
    "      + \"ręcznie\" liczona kara (lub bonus=0)\n",
    "    \"\"\"\n",
    "    # 1) Obliczamy CE (per sample, a nie średnią)\n",
    "    ce = F.cross_entropy(layout_logits, true_layout, reduction='none')  # [batch_size]\n",
    "\n",
    "    # 2) Predykcja layoutu (argmax).\n",
    "    layout_pred = torch.argmax(layout_logits, dim=1)  # [batch_size]\n",
    "\n",
    "    # 3) Szacujemy prawdopodobieństwo kliknięcia p\n",
    "    #    w zależności od X i *wybranego* layoutu\n",
    "    p = click_model(X_batch, layout_pred).squeeze(1)  # [batch_size]\n",
    "    p = torch.clamp(p, min=1e-5, max=1-1e-5)  # minimalna stabilizacja\n",
    "\n",
    "    # 4) Obliczamy karę według zadanych reguł\n",
    "    correct = (layout_pred == true_layout)  # tensor bool [batch_size]\n",
    "    \n",
    "    # Inicjujemy zerowy wektor kary\n",
    "    penalty = torch.zeros_like(ce)\n",
    "\n",
    "    # A) layout_pred != layout_true => kara = 1/p\n",
    "    mask_incorrect = (~correct)\n",
    "    penalty[mask_incorrect] = 1.0 / p[mask_incorrect]\n",
    "\n",
    "    # B) layout_pred == layout_true i clicked=0 => kara=1\n",
    "    #    layout_pred == layout_true i clicked=1 => kara=0\n",
    "    mask_correct_zero = (correct & (clicked==0))\n",
    "    penalty[mask_correct_zero] = 1.0\n",
    "    \n",
    "    # mask_correct_one = (correct & (clicked==1)) => kara pozostaje = 0\n",
    "\n",
    "    total_loss_per_sample = ce + penalty * lambda_val\n",
    "\n",
    "    return total_loss_per_sample.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "699a83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20, Training Loss: 11.9942\n",
      "Epoch  2/20, Training Loss: 11.9941\n",
      "Epoch  3/20, Training Loss: 11.9941\n",
      "Epoch  4/20, Training Loss: 11.9941\n",
      "Epoch  5/20, Training Loss: 11.9941\n",
      "Epoch  6/20, Training Loss: 11.9941\n",
      "Epoch  7/20, Training Loss: 11.9941\n",
      "Epoch  8/20, Training Loss: 11.9942\n",
      "Epoch  9/20, Training Loss: 11.9941\n",
      "Epoch 10/20, Training Loss: 11.9941\n",
      "Epoch 11/20, Training Loss: 11.9941\n",
      "Epoch 12/20, Training Loss: 11.9942\n",
      "Epoch 13/20, Training Loss: 11.9941\n",
      "Epoch 14/20, Training Loss: 11.9942\n",
      "Epoch 15/20, Training Loss: 11.9942\n",
      "Epoch 16/20, Training Loss: 11.9941\n",
      "Epoch 17/20, Training Loss: 11.9942\n",
      "Epoch 18/20, Training Loss: 11.9941\n",
      "Epoch 19/20, Training Loss: 11.9941\n",
      "Epoch 20/20, Training Loss: 11.9941\n",
      "\n",
      "Validation Loss: 11.2924\n",
      "L_score na zbiorze walidacyjnym: 0.1016\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Wczytanie i przygotowanie danych ---\n",
    "\n",
    "X_train_scaled = pd.read_csv(\"data/feature_engineering/processed_x_train.csv\")\n",
    "X_val_scaled = pd.read_csv(\"data/feature_engineering/processed_x_valid.csv\")\n",
    "y_train = pd.read_csv(\"data/y_train.csv\")\n",
    "y_val = pd.read_csv(\"data/y_valid.csv\")\n",
    "\n",
    "# Tworzymy zestawy danych i DataLoadery\n",
    "train_dataset = HackatonDataset(X_train_scaled, y_train)\n",
    "val_dataset   = HackatonDataset(X_val_scaled, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# --- 3. Definicja modelu (logistyczna regresja) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LayoutModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 300),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(300, num_classes),\n",
    "            nn.Softmax(dim=1)  # Używamy Softmax na końcu, aby uzyskać prawdopodobieństwa\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "num_classes = 10  # Zakładamy 10 unikalnych wartości layout_type\n",
    "model = LayoutModel(input_dim, num_classes)\n",
    "\n",
    "# Wybieramy optymalizator\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.032)\n",
    "\n",
    "# --- 4. Definicja customowego lossu ---\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ...\n",
    "# Załóżmy, że mamy wczytane:\n",
    "#   X_train_scaled, X_val_scaled   (DataFrame lub gotowe macierze)\n",
    "#   y_train, y_val                 (DataFrame z kolumnami: 'label' i 'clicked')\n",
    "\n",
    "train_dataset = HackatonDataset(X_train_scaled, y_train)\n",
    "val_dataset   = HackatonDataset(X_val_scaled, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=False)\n",
    "\n",
    "# Definiujemy modele\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "layout_model = LayoutModel(input_dim, 10)\n",
    "click_model  = ClickModel(input_dim)\n",
    "\n",
    "# Optymalizator z parametrami obu modeli\n",
    "optimizer = optim.Adam(\n",
    "    list(layout_model.parameters()) + list(click_model.parameters()),\n",
    "    lr=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    layout_model.train()\n",
    "    click_model.train()\n",
    "    \n",
    "    all_losses = []\n",
    "    for X_batch, y_batch, clicked_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # forward layout\n",
    "        layout_logits = layout_model(X_batch)\n",
    "        # nasz custom loss\n",
    "        loss = custom_loss_fn(\n",
    "            layout_logits, \n",
    "            y_batch, \n",
    "            clicked_batch,\n",
    "            X_batch,\n",
    "            click_model,\n",
    "            lambda_val=10\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_losses.append(loss.item())\n",
    "   \n",
    "\n",
    "# --- Walidacja ---\n",
    "layout_model.eval()\n",
    "click_model.eval()\n",
    "val_losses = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch, clicked_batch in val_loader:\n",
    "        layout_logits = layout_model(X_batch)\n",
    "        loss = custom_loss_fn(layout_logits, y_batch, clicked_batch, X_batch, click_model, lambda_val=10)\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        pred_layout = torch.argmax(layout_logits, dim=1)\n",
    "        all_preds.extend(pred_layout.cpu().numpy())\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "# --- 5. Pętla treningowa wykorzystująca custom loss ---\n",
    "\n",
    "lambda_val = 10  # Ustalona wartość parametru lambda – można eksperymentować\n",
    "num_epochs = 20\n",
    "\n",
    "######### mozliwe ze tutaj do zmiany\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch, clicked_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = custom_loss_fn(logits, y_batch, clicked_batch, X_batch, click_model, lambda_val)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1:2d}/{num_epochs}, Training Loss: {np.mean(epoch_losses):.4f}\")\n",
    "    \n",
    "    ######### mozliwe ze tutaj do zmiany - koniec\n",
    "\n",
    "# --- 6. Walidacja modelu ---\n",
    "\n",
    "model.eval()\n",
    "val_losses = []\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_clicked = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        X_batch, y_batch, clicked_batch = batch\n",
    "        logits = model(X_batch)\n",
    "        loss = custom_loss_fn(logits, y_batch, clicked_batch, X_batch, click_model, lambda_val)\n",
    "        val_losses.append(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(y_batch.cpu().numpy())\n",
    "        all_clicked.extend(clicked_batch.cpu().numpy())\n",
    "        \n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nValidation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# --- 7. Ocena modelu za pomocą metryki L_score ---\n",
    "\n",
    "\n",
    "# Przygotowanie zbioru walidacyjnego do obliczania L_score\n",
    "val_df = y_val.copy()\n",
    "# Dodajemy kolumnę z predykcjami uzyskanymi z modelu\n",
    "val_df['y_pred'] = all_preds\n",
    "\n",
    "l_score_val = L_score(val_df[['label','clicked']], val_df['y_pred'])\n",
    "print(f\"L_score na zbiorze walidacyjnym: {l_score_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combainslot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
